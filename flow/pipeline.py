from config.log_config import logger_config
from src.extract.csv_extractor import ExtractorCSV
from src.transform.cleaner import DataCleaner
from src.load.db_loader import DBLoader
from src.utils.data_profiler import DataProfiler

logger = logger_config('flow.pipeline')

class ETLPipeline:
    """Main ETL Pipeline orchestrator"""
    
    # T·∫°o h√†m __init__ ƒë·ªÉ kh·ªüi t·∫°o pipeline v·ªõi ƒë∆∞·ªùng d·∫´n csv 
    # Khi g·ªçi t·ªõi class ETLPipeline th√¨ s·∫Ω ph·∫£i truy·ªÅn v√†o ƒë∆∞·ªùng d·∫´n csv ƒë·ªÉ pipeline bi·∫øt ƒë∆∞·ª£c ngu·ªìn d·ªØ li·ªáu ·ªü ƒë√¢u
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
    
    def run(self):
        """Execute full ETL pipeline"""
        try:
            logger.info("=" * 60)
            logger.info("üöÄ Starting ETL Pipeline")
            logger.info("=" * 60)
            
            # Step 1: Setup tables
            logger.info("Step 1: Creating tables...")
            DBLoader.create_raw_and_clean_table()
            logger.info("‚úÖ Tables ready")
            
            # Step 2: Extract
            logger.info("Step 2: Extracting data...")
            df_raw = ExtractorCSV.extract(self.csv_path)
            logger.info(f"‚úÖ Extracted {len(df_raw)} rows")
            
            # Step 3: Load raw
            logger.info("Step 3: Loading raw data...")
            DBLoader.load_to_raw_table(df_raw, self.csv_path, skip_if_exist=True)
            logger.info("‚úÖ Raw data loaded")
            logger.info("=="*60)

            # Step 4: Transform
            logger.info("Step 4: Transforming data...")
            df_clean = DataCleaner.clean_data(df_raw)
            logger.info(f"‚úÖ Cleaned to {len(df_clean)} rows")
            logger.info("=="*60)
            
            # Step 5: Load clean
            logger.info("Step 5: Loading clean data...")
            DBLoader.load_to_clean_table(df_clean, self.csv_path, skip_if_exist=True)
            logger.info("‚úÖ Clean data loaded")
            logger.info("=="*60)
            
            # Step 6: Generate report
            logger.info("Step 6: Generating quality report...")
            report = DataProfiler.generated_quantity_report()
            logger.info("=" * 60)

            logger.info("‚úÖ ETL Pipeline Completed Successfully!")
            logger.info("=" * 60)
            logger.info("üìà Quality Report:")
            logger.info(f"  Raw Records: {report['raw_record']}")
            logger.info(f"  Clean Records: {report['clean_record']}")
            logger.info(f"  Records Dropped: {report['record_dropped']} ({report['drop_rate']})")
            logger.info(f"  Drop Rate: {report['drop_rate']:.2f}%")
            logger.info(f"  Unique Models: {report['unique_model']}")
            logger.info(f"  Price Range: ${report['price_stat']['min']:,} - ${report['price_stat']['max']:,}")
            logger.info("=" * 60)
            
            return report
            
        except Exception as e:
            logger.exception(f"‚ùå ETL Pipeline failed: {e}")
            raise